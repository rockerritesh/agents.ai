{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd7f550",
   "metadata": {},
   "source": [
    "# v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9cfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37f1e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open yaml\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "def load_yaml(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: The file {file_path} does not exist.\")\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "            return data\n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"Error parsing YAML file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef7a4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_yaml('agents_behaviour.yaml')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0419df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7bbd7124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the agent's name and description\n",
    "def extract_agent_info(agent):\n",
    "    name = agent.get('name', 'Unknown')\n",
    "    description = agent.get('description', 'No description provided')\n",
    "    return name, description\n",
    "\n",
    "# Extract information for each agent\n",
    "agent_info = []\n",
    "for agent in data:\n",
    "    name, description = extract_agent_info(agent)\n",
    "    agent_info.append({'name': name, 'description': description})\n",
    "# Display the extracted information\n",
    "# for info in agent_info:\n",
    "#     print(f\"Agent Name: {info['name']}\")\n",
    "#     print(f\"Description: {info['description']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ed0f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tools_details = []\n",
    "\n",
    "# list all the tools used by the agents\n",
    "def extract_tools(agent):\n",
    "    return agent.get('tools', [])\n",
    "\n",
    "# Extract tools for each agent\n",
    "for agent in data:\n",
    "    tools = extract_tools(agent)\n",
    "    # print(tools)\n",
    "    for tool in tools:\n",
    "        # print(tool['name'], tool['description'])\n",
    "        tools_details.append({'name': tool['name'], 'description': tool['description']})\n",
    "\n",
    "# for tool in tools_details:\n",
    "#     print(f\"Tool Name: {tool['name']}\")\n",
    "#     print(f\"Description: {tool['description']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "03104dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apicall import get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8277b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_embeddings = []\n",
    "# Generate embeddings for each agent's description\n",
    "for agent in agent_info:\n",
    "    embedding = get_embedding(f\"Name:{agent['name']}.\\n Description: {agent['description']}\")\n",
    "    agent_embeddings.append({'name': agent['name'], 'embedding': embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16d814dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45f1af5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'EmojiWriter', 'similarity': np.float64(0.2870189736889936)},\n",
       " {'name': 'TranslatorAgent', 'similarity': np.float64(0.1250715121941039)},\n",
       " {'name': 'TextSummarizer', 'similarity': np.float64(0.10902902061560518)},\n",
       " {'name': 'CSVDataAgent', 'similarity': np.float64(0.0632980593235556)},\n",
       " {'name': 'PDFContentWriter', 'similarity': np.float64(0.05371273767444446)}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tools import CosineSimilarityCalculator\n",
    "query = \"A fat man on the rat, emojify this sentence\"\n",
    "\n",
    "query_embedding = get_embedding(query)\n",
    "# Find the most relevant agent based on the query embedding and return the agent name and simillarity\n",
    "def find_relevant_agent(query_embedding, agent_embeddings):\n",
    "    sim_score = []\n",
    "    for agent in agent_embeddings:\n",
    "        embedding_agent = agent['embedding']\n",
    "        # print(embedding_agent.shape, query_embedding.shape)\n",
    "        similarity = CosineSimilarityCalculator.calculate_similarity(embedding1 = query_embedding, embedding2 = embedding_agent)\n",
    "        sim_score.append({'name': agent['name'], 'similarity': similarity})\n",
    "    # Sort agents by similarity score in descending order\n",
    "    sim_score.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    return sim_score\n",
    "relevant_agent = find_relevant_agent(query_embedding, agent_embeddings)\n",
    "# print(f\"Most relevant agent for the query '{query}':\")\n",
    "relevant_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5ddf3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant_agent[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "45f2e82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools for the most relevant agent 'EmojiWriter':\n",
      "Tool Name: EmojiTranslator\n",
      "Description: Translates words and concepts to relevant emojis\n",
      "\n",
      "Tool Name: EmojiMixer\n",
      "Description: Creates custom emoji combinations for content\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list the tools for a agent\n",
    "def list_tools_for_agent(agent_name, data):\n",
    "    for agent in data:\n",
    "        if agent['name'] == agent_name:\n",
    "            return agent.get('tools', [])\n",
    "    return []\n",
    "# Get tools for the most relevant agent\n",
    "tools_for_relevant_agent = list_tools_for_agent(relevant_agent[0]['name'], data)\n",
    "# Display the tools for the most relevant agent\n",
    "print(f\"Tools for the most relevant agent '{relevant_agent[0]['name']}':\")\n",
    "for tool in tools_for_relevant_agent:\n",
    "    print(f\"Tool Name: {tool['name']}\")\n",
    "    print(f\"Description: {tool['description']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "06a69294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolsSeqFinder(tools_name_in_seq=['EmojiTranslator', 'EmojiMixer'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from apicall import get_reply\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You have to find the best sequence for list of tool to complete the task. List of tools are {tools_for_relevant_agent}\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I want to extract the text from pdf and refactor the text\"\n",
    "    }\n",
    "]\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ToolsSeqFinder(BaseModel):\n",
    "    tools_name_in_seq: list[str]\n",
    "\n",
    "\n",
    "tools_order = get_reply(message,ToolsSeqFinder )\n",
    "tools_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e8756aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<class 'str'>\"}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the input schema for the tools function\n",
    "import inspect\n",
    "from tools import TextExtractor, ContentReformatter, EmojiTranslator,EmojiMixer\n",
    "\n",
    "def print_input_schema(func):\n",
    "    signature = inspect.signature(func)\n",
    "    # print(f\"Input schema for {func.__name__}:\")\n",
    "    function_dict = {}\n",
    "    for param in signature.parameters.values():\n",
    "        # print(f\"{param.name}: {param.annotation}\")\n",
    "        function_dict[param.name] = str(param.annotation)\n",
    "    return function_dict\n",
    "func_dict = print_input_schema(EmojiTranslator.translate_to_emoji)\n",
    "func_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d319a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4469/3942844028.py:29: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  reply = reply.dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information_tillnow='The user needs to extract and summarize text from a PDF file.' all_information_gathered=False flow_of_question='Ask about the tools or software the user plans to use for extracting and summarizing the text from the PDF.'\n",
      "information_tillnow='' all_information_gathered=False flow_of_question='What is the main topic or content of the PDF? This helps in understanding what kind of summary is expected.'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ask the user to input the tools required to complete the task\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You have to ask for the details of the tools required to complete the task. The tools required are {func_dict}\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I want to extract the text from pdf and summarize the text.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "class ToolsInput(BaseModel):\n",
    "    information_tillnow: str\n",
    "    all_information_gathered: bool\n",
    "    flow_of_question: str\n",
    "\n",
    "history = ''\n",
    "\n",
    "# set False for message['all_information_gathered'] to continue the flow of question\n",
    "reply = {}\n",
    "reply['all_information_gathered'] = False\n",
    "\n",
    "while True:\n",
    "    # Get the reply from the user\n",
    "    reply = get_reply(message, ToolsInput)\n",
    "    print(reply)\n",
    "    # load as json\n",
    "    reply = reply.dict()\n",
    "    if reply['all_information_gathered']:\n",
    "        break\n",
    "    else:\n",
    "        # Update the message with the flow of question\n",
    "        history = history + reply['flow_of_question'] + '\\n'\n",
    "        message[0]['content'] = f\"You have to ask for the details required to complete the task. The tools required are {func_dict}. History of questions: {history}\"\n",
    "        query = input(\"Please provide the details for the tool: \")\n",
    "        message[1]['content'] = query\n",
    "\n",
    "# Print the final details gathered for the tools\n",
    "print(\"Final details gathered for the tools:\")\n",
    "print(f\"Required Details: {reply['information_tillnow']}\")\n",
    "print(f\"All Information Gathered: {reply['all_information_gathered']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert user input to the function input\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You have to convert the user input to the function input. The function input is {func_dict}\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Convert the user input to the function input as json. The user input is {reply['information_tillnow']} return it as a json object.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "class FunctionInput(BaseModel):\n",
    "    function_input: str\n",
    "\n",
    "function_input = get_reply(message, FunctionInput)\n",
    "# Print the function input\n",
    "# print(\"Function Input:\")\n",
    "# print(function_input.function_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479add4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted text from resume.pdf\n"
     ]
    }
   ],
   "source": [
    "text = TextExtractor.extract_text_from_pdf(pdf_path=function_input.function_input)\n",
    "# Print the extracted text\n",
    "# print(\"Extracted Text:\")\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a415eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolsSeqFinder(tools_name_in_seq=['TextExtractor', 'ContentReformatter'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f6a09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<class 'str'>\", 'format_description': \"<class 'str'>\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_dict = print_input_schema(ContentReformatter.reformat_content)\n",
    "func_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c5a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59356/1840275156.py:29: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  reply = reply.dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information_tillnow='Text has been provided and needs to be reformatted.' all_information_gathered=False flow_of_question='Do you need a specific format for the reformatted text?'\n",
      "information_tillnow='The user appears to have provided a detailed text including personal information, professional summary, skills, education, work experience, publications, projects, honors and awards, and social experience.' all_information_gathered=False flow_of_question='What specific markdown format or structure do you require for presenting this information?'\n",
      "information_tillnow='The user needs a markdown format for the provided text.' all_information_gathered=False flow_of_question='What specific sections or headings should the markdown document contain? Do you want any particular styling or features such as bullet points, tables, or links included?'\n",
      "information_tillnow='- Extracted text contains personal information, summary, core competencies, education details, skills, experience, publications, projects, honors, awards, and social experience. \\n- No specific format for JSON format was discussed.' all_information_gathered=False flow_of_question='Should the JSON format include specific keys for each section, and do you require any specific structure or nesting for the JSON data?'\n",
      "information_tillnow='You need the extracted text to be reformatted into a specific JSON structure with keys and values corresponding to sections of the document.' all_information_gathered=False flow_of_question=\"What are the specific keys you want for each section of the JSON document, such as 'contact_information', 'experience', or 'projects'? Do you require any specific data types or structure for these keys, like arrays for lists of items?\"\n",
      "information_tillnow='I have gathered that the task requires converting the extracted text into a markdown and JSON format with specific structure and keys like contact_information, experience, education, etc. There is also mention of utilizing bullet points and potentially other formatting features.' all_information_gathered=False flow_of_question='What specific details or additional preferences do you have for the JSON file structure and keys? Is there any specific nesting of data you would like within the JSON or any particular formats for dates and text sections?'\n",
      "information_tillnow=\"You require a simple JSON structure for the data provided, however, there isn't specific guidance on how to structure the JSON keys and values.\" all_information_gathered=False flow_of_question='What specific keys do you want in the JSON document, and is there a preferred structure or hierarchy to follow?'\n",
      "information_tillnow='The user has provided a text and asked for a format without keys and values at a single level.' all_information_gathered=True flow_of_question='Understand the specific requirements for the text formatting without using key-value pairs.'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ask the user to input the tools required to complete the task\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You have to ask for the details of the tools required to complete the task. The tools required are {func_dict}\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"I want to extract the text from pdf and reformat the text. The text is already extracted. And the text is {text}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "class ToolsInput(BaseModel):\n",
    "    information_tillnow: str\n",
    "    all_information_gathered: bool\n",
    "    flow_of_question: str\n",
    "\n",
    "history = ''\n",
    "\n",
    "# set False for message['all_information_gathered'] to continue the flow of question\n",
    "reply = {}\n",
    "reply['all_information_gathered'] = False\n",
    "\n",
    "while True:\n",
    "    # Get the reply from the user\n",
    "    reply = get_reply(message, ToolsInput)\n",
    "    print(reply)\n",
    "    # load as json\n",
    "    reply = reply.dict()\n",
    "    if reply['all_information_gathered']:\n",
    "        break\n",
    "    else:\n",
    "        # Update the message with the flow of question\n",
    "        history = history + reply['flow_of_question'] + '\\n'\n",
    "        message[0]['content'] = f\"You have to ask for the details required to complete the task. The tools required are {func_dict}. History of questions: {history}. The text is already extracted. And the text is {text}\"\n",
    "        query = input(\"Please provide the details for the tool. The tools required are {func_dict} \")\n",
    "        message[1]['content'] = query\n",
    "\n",
    "# Print the final details gathered for the tools\n",
    "# print(\"Final details gathered for the tools:\")\n",
    "# print(f\"Required Details: {reply['information_tillnow']}\")\n",
    "# print(f\"All Information Gathered: {reply['all_information_gathered']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2cdbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Input:\n",
      "Sumit Yadav\n",
      "Email: rockerritesh4@gmail.com\n",
      "Portfolio: sumityadav.com.np\n",
      "Mobile:\n",
      "+977-9819856148\n",
      "Github:\n",
      "github.com/rockerritesh\n",
      "LinkedIn:\n",
      "linkedin.com/in/rockerritesh\n",
      "Summary\n",
      "AI Engineer specializing in natural language processing (NLP) and AI optimization, with 5+ years of\n",
      "experience developing production-grade AI systems. Currently architecting:\n",
      "• Multi-agent RAG systems with guardrails for secure information retrieval.\n",
      "• Context-aware chatbots with post-conversation analysis capabilities.\n",
      "• LLM evaluation frameworks for accuracy and reliability testing.\n",
      "• MCP Server for easy and fast way to integrate Agents.\n",
      "Proven track record in AI/Machine Learning engineering across many NLP projects including maithili text\n",
      "classification(low-resources) (0.87 accuracy) and multilingual document analysis systems. Authored 3 peer-reviewed\n",
      "publications and one open review paper on machine learning optimization/security and low-resource language\n",
      "processing\n",
      "Core Competencies:\n",
      "• AI Prompt Design • LLM Fine-tuning • Security Document Analysis\n",
      "• Technical Documentation • Cross-functional Collaboration • GRC Data Annotation\n",
      "Education\n",
      "• Pulchowk Engineering College\n",
      "Kathmandu, Nepal\n",
      "Bachelor of Computer Engineering\n",
      "Courses: SDNs, FinTech, Operating Systems, Data Structures, Big Data, Artificial Intelligenc, Networking, Databases\n",
      "Skills Summary\n",
      "• Languages:\n",
      "Python, C, C++, Bash\n",
      "• Online Courses:\n",
      "Deep Learning and GAN Specialization, Generative AI LLM, Image Understanding TensorFlow GCP\n",
      "• Tools/Module:\n",
      "CI//CD, GIT,Pytorch,LangChain, LlamaIndex,Django, streamlit, MySQL, GraphQL\n",
      "• Soft Skills:\n",
      "Leadership, Event Management, Writing, Public Speaking, Time Management\n",
      "• Hobbies: Walking, Meditation, Deep Think, Meta-THinking.\n",
      "Experience\n",
      "• Amnil Technology Pvt. Ltd\n",
      "Lalitpur\n",
      "AI Engineer (Full-time)\n",
      "May 2024 - Now\n",
      "◦Generative AI and Machine Learning Engineering: On Project related to RAG, Agent based, recursive query,\n",
      "Chatbot, SQL Agent, and scheduling optimization. Made the system like Guardrails, LLM evaluation and Report\n",
      "generation.\n",
      "◦LLM hosting, inference optimization, and API integration.: I hosted different embedding model and the\n",
      "completion model eg.LLaMA 3.3 3B model on server using the vLLM inference engine, ensuring efficient performance\n",
      "and easy API integration.\n",
      "• Ed-Acadia\n",
      "Lalitpur\n",
      "Chief Data Oficer (Full-time)\n",
      "May 2022 - 2023\n",
      "◦AI/ML Projects: Supervising the project and research related to Data Science. Works of different DocumentsAI\n",
      "system for low resources language.\n",
      "• PDSC(Plan Design Solve Create)\n",
      "Lalitpur\n",
      "Software Coordinator (Full-time)\n",
      "May 2022 - 2023\n",
      "◦Project Management: Supervising the project and research related to Data Science.\n",
      "• DeepLearning.AI\n",
      "Virtual\n",
      "GAN Mentor (Part-time)\n",
      "Aug 2021 - Present\n",
      "◦Course - GAN Specialization: Helping the student in understanding the key concept behind Unsupervised learning\n",
      "(GAN).\n",
      "• Robotics Association of Nepal\n",
      "Lalitpur\n",
      "AI and Robotics Member (Part-time)\n",
      "2021 - Present\n",
      "◦Making Robotics based system: Done research and project related to Computer Vision based on raspberrypi\n",
      "microcontroller.\n",
      "Publications\n",
      "• SUPPORT VECTORS ARE A BETTER WAY OF TEXT CLASSIFICATION FOR IMBALANCED DATA:\n",
      "Present a robust SVC method for text classification (100+ classes) using term-frequency vectorization, achieving superior test\n",
      "data results over neural networks.\n",
      "• Machine Learning Analysis of Tirhuta Lipi: Achieved 0.97 accuracy in Tirhuta Lipi character recognition using\n",
      "MobileNet embedding and logistic regression, with applications in translation and OCR for low-resource languages.\n",
      "• Revolutionizing Currency Security: A Yolov8-Based Approach for Automated Detection of Counterfeit Nepali\n",
      "Banknotes: Implemented YOLOv8 to achieve a true positive recall of 0.82 (front face) and 0.9863 (back face) in detecting\n",
      "counterfeit Nepali banknotes, demonstrating significant advancements in counterfeit currency detection.\n",
      "Projects\n",
      "• Vibe-Coder:\n",
      "Made an Agent that will do Streamlit and FastAPI. Tech: Agent, MCP, Claude API keys, Python, Streamlit\n",
      "• Retrieval Augmentation Generation System (RAG) and Intelligent Document Processing(IDP): Developed a\n",
      "retrieval-augmented reality system for enhanced information access and interaction. Tech: OpenAI, Gemini, Claude API keys,\n",
      "Python.\n",
      "• Nepali Chat with Doc: Implemented a chatbot for Nepali language using Devanagari and Preeti fonts. Features include\n",
      "Guardrails system, post-conversation analysis, and agent-based systems like SQL Agent, Excel Agent, and Reflexive Agents.\n",
      "Preeti to Unicode Conversion. Tech: OpenAI, Gemini, Claude API keys.\n",
      "• Bachelor’s Major Project: Evaluating Auto-Encoder Transformer Language Model for Maithili Text\n",
      "Classification: Established a benchmark in this language. First to create a corpus in Devanagari Maithili language, trained\n",
      "LLM for Maithili, and performed downstream task classification. Tech: LLM, Transformer(bert), Pytorch, Streamlit & Big\n",
      "Data. (April ’2024)\n",
      "• IRB (Image Recognition Based) Robotics Arm (Image Processing, Signal Processing, Actuator Control):\n",
      "Research-oriented, open-source project under UN’s SDG3 - Good Health & Well-Being. Tech: Python, Arduino Programming,\n",
      "Arduino Toolkit, TensorFlow (May ’2020).\n",
      "• Nepali Language Projects: Developed multiple applications, including a Devanagari letter classifier using VGG16 (accuracy\n",
      "0.94), a Nepali sentiment analysis model, and a simple OCR for Nepali text. Tech: Keras, Transformer, Pytorch, TF-IDF,\n",
      "NLTK. (Past 2 Years)\n",
      "• Unsupervised Model: Explored the behavior of latent spaces using VAE, GAN, C-GAN, AC-GAN, and DC-GAN. Tech:\n",
      "Python, Numpy, TensorFlow. (Sep, 2021)\n",
      "• NEPSE Simple: Presented Nepal stock market data in a minimal environment constraint. Tech: GitHub Workflow,\n",
      "Automation in Scraping, WebSockets, JavaScript, RSS, XML. (Since 2020)\n",
      "• Advanced Document and AI Systems: Designed and implemented a variety of tools, including:\n",
      "◦Chat systems for Nepali and multilingual documents with Preeti-to-Unicode conversion and guardrails for improved user\n",
      "interaction.\n",
      "◦AI-powered memo creation and advanced Excel file manipulation tools.\n",
      "◦Contract document analysis using recursive and advanced reasoning GPT systems.\n",
      "◦Translation systems for Nepali documents using OCR and text conversion.\n",
      "◦Chat and interaction systems for image and audio data with TTS and Whisper integration.\n",
      "• Verification and Financial Prediction Systems: Developed:\n",
      "◦A face and signature verification app using VGG-based advanced face detection and liveness detection algorithms.\n",
      "◦A loan eligibility prediction system utilizing knowledge-based reasoning techniques.\n",
      "Honors and Awards\n",
      "• Winner of GritFeat AI Hackathon 2023, Locus - Feb, 2023,(SWIFT’ is a wearable devices with hardware and AI\n",
      "models that detect falls in elderly people with 0.7986 accuracy, resulting in immediate emergency alerts to\n",
      "contacts.)\n",
      "• First RunnerUP of Dataverse, Locus - Jan, 2023,Dataverse Solution (NLP pased problem to classify abstract.)\n",
      "• Winner of Best AI Project of Deltathon, DELTA 3.0 - Jan, 2022,Nepali Harvest (Designed a portal to help farmers\n",
      "that can predicting diseases, identifying optimal harvest times, and aiding with crop health assessment.)\n",
      "• Winner of Image Challenge, IT-Meet UP KU - Sep, 2022 (Have to train AI model to classify image of Ballot paper.)\n",
      "• Winner of Capture The Flag, LogPoint - Feb, 2021 (Tasked of finding information and exploiting a binary file.)\n",
      "• Runner’s Up at DATARUSH by DOCSUMO - Feb, 2021 (NLP based model for classifying Abstract into Classes.)\n",
      "Social Experience\n",
      "• Team of NPL Coders\n",
      "Global\n",
      "Conducted National level Data Science Coding Compition on Kaggle and Hacker Ranks.\n",
      "Sep 2023 - Present\n",
      "• Joint Secretary at NTBNS Student Clubs, IOE, Pulchowk Campus\n",
      "Lalitpur, Nepal\n",
      "Conducted technical training & Organized nepal largest sarswati puja Program.\n",
      "Jan 2020 - Present\n",
      "• Tutor of Children In Technology- WorldLink\n",
      "Nepal\n",
      "Aware the student about Risk and Safety of Internet.\n",
      "Nov 2023\n",
      "\n",
      "format_description: without keys and values at a single level\n"
     ]
    }
   ],
   "source": [
    "# convert user input to the function input\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You have to convert the user input to the function input. The function input is {func_dict} and the text is {text}\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Convert the user input to the function input as json. The user input is {reply['information_tillnow']} return it as a json object.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "class FunctionInput(BaseModel):\n",
    "    function_input: str\n",
    "\n",
    "function_input = get_reply(message, FunctionInput)\n",
    "# Print the function input\n",
    "print(\"Function Input:\")\n",
    "print(function_input.function_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75fb645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64957ff8",
   "metadata": {},
   "source": [
    "# v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d1ae3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import inspect\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import numpy as np\n",
    "\n",
    "# Import custom modules\n",
    "from apicall import get_embedding, get_reply, Reply\n",
    "from tools import (\n",
    "    DataframeLoader, ContentExtractor, EmojiTranslator, EmojiMixer,\n",
    "    KeypointExtractor, ContentExpander, TextExtractor, ContentReformatter,\n",
    "    MultilingualTranslator, CosineSimilarityCalculator,StylePreserver\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a7819e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load YAML configuration file\n",
    "def load_yaml(file_path):\n",
    "    \"\"\"Load and parse a YAML file.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: The file {file_path} does not exist.\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "            return data\n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"Error parsing YAML file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2596690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the agent configuration from YAML\n",
    "agents_config = load_yaml('agents_behaviour.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c711fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract agent information\n",
    "class AgentInfo(BaseModel):\n",
    "    \"\"\"Model for storing agent information.\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    id: str\n",
    "    tools: List[Dict[str, Any]]\n",
    "    parameters: Optional[Dict[str, Any]] = None\n",
    "    embedding: Optional[Any] = None\n",
    "\n",
    "def extract_agent_info(agents_data):\n",
    "    \"\"\"Extract detailed information for each agent.\"\"\"\n",
    "    agents = []\n",
    "    for agent_data in agents_data:\n",
    "        agent = AgentInfo(\n",
    "            name=agent_data.get('name', 'Unknown'),\n",
    "            description=agent_data.get('description', 'No description provided'),\n",
    "            id=agent_data.get('id', 'unknown-id'),\n",
    "            tools=agent_data.get('tools', []),\n",
    "            parameters=agent_data.get('parameters', {})\n",
    "        )\n",
    "        agents.append(agent)\n",
    "    return agents\n",
    "\n",
    "# Create agent information objects\n",
    "agents = extract_agent_info(agents_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a67407ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentInfo(name='CSVDataAgent', description='Specialized agent for loading, processing, and extracting insights from CSV data files', id='agent-010', tools=[{'name': 'DataframeLoader', 'version': '1.2.0', 'description': 'Loads CSV files into pandas dataframes for content extraction', 'usage': 'Use this tool first when working with CSV data files', 'input_schema': {'file_path': 'str'}, 'output_schema': 'pd.DataFrame', 'capabilities': ['csv-import', 'excel-import', 'data-cleaning']}, {'name': 'ContentExtractor', 'version': '1.1.5', 'description': 'Extracts narrative content and insights from structured dataframes', 'usage': 'Use this tool after DataframeLoader to generate narratives from data', 'input_schema': {'dataframe': 'pd.DataFrame', 'columns': 'List[str]', 'sample_rows': 'int'}, 'output_schema': 'str', 'capabilities': ['data-to-text', 'insight-generation', 'story-formatting']}], parameters={'max_file_size': '50MB', 'writing_style': 'informative'}, embedding=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5310bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each agent based on name and description\n",
    "def generate_agent_embeddings(agents_list):\n",
    "    \"\"\"Generate embeddings for each agent based on their name and description.\"\"\"\n",
    "    for agent in agents_list:\n",
    "        embedding = get_embedding(f\"Name: {agent.name}.\\nDescription: {agent.description}\")\n",
    "        agent.embedding = embedding\n",
    "    return agents_list\n",
    "\n",
    "# Generate embeddings for all agents\n",
    "agents_with_embeddings = generate_agent_embeddings(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b204e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentInfo(name='CSVDataAgent', description='Specialized agent for loading, processing, and extracting insights from CSV data files', id='agent-010', tools=[{'name': 'DataframeLoader', 'version': '1.2.0', 'description': 'Loads CSV files into pandas dataframes for content extraction', 'usage': 'Use this tool first when working with CSV data files', 'input_schema': {'file_path': 'str'}, 'output_schema': 'pd.DataFrame', 'capabilities': ['csv-import', 'excel-import', 'data-cleaning']}, {'name': 'ContentExtractor', 'version': '1.1.5', 'description': 'Extracts narrative content and insights from structured dataframes', 'usage': 'Use this tool after DataframeLoader to generate narratives from data', 'input_schema': {'dataframe': 'pd.DataFrame', 'columns': 'List[str]', 'sample_rows': 'int'}, 'output_schema': 'str', 'capabilities': ['data-to-text', 'insight-generation', 'story-formatting']}], parameters={'max_file_size': '50MB', 'writing_style': 'informative'}, embedding=array([-0.03066171, -0.04836608, -0.01594289, ..., -0.02179459,\n",
       "       -0.00664287, -0.03200521], shape=(3072,)))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents_with_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7045833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the most relevant agent based on a query\n",
    "def find_relevant_agents(query, agents_list, top_n=3):\n",
    "    \"\"\"\n",
    "    Find the most relevant agents based on the query embedding.\n",
    "    Returns the top N agents sorted by similarity score.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    sim_scores = []\n",
    "    \n",
    "    for agent in agents_list:\n",
    "        similarity = CosineSimilarityCalculator.calculate_similarity(\n",
    "            embedding1=query_embedding, \n",
    "            embedding2=agent.embedding\n",
    "        )\n",
    "        sim_scores.append({\n",
    "            'agent': agent,\n",
    "            'similarity': similarity\n",
    "        })\n",
    "    \n",
    "    # Sort agents by similarity score in descending order\n",
    "    sim_scores.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    # if greates similarity is less than 0.3, return empty list\n",
    "    if sim_scores[0]['similarity'] < 0.3:\n",
    "        class QA(BaseModel):\n",
    "            \"\"\"Model for storing query and answer.\"\"\"\n",
    "            answer: str\n",
    "        message = [\n",
    "           \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Query: {query}. No relevant agent found.\"\n",
    "            }\n",
    "        ]\n",
    "        reply = get_reply(message, QA)\n",
    "        print(reply.answer)\n",
    "        return []\n",
    "\n",
    "    return sim_scores[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cb9cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I want to extract the text from a PDF and summarize it.\"\n",
    "\n",
    "agents_with_embeddings = find_relevant_agents(query, agents_with_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb6a3c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'agent': AgentInfo(name='TextSummarizer', description='Specialized agent for extracting key points from text and expanding summaries into full content', id='agent-012', tools=[{'name': 'KeypointExtractor', 'version': '1.3.0', 'description': 'Identifies key points and main ideas from text content', 'usage': 'Use this tool to extract key points from lengthy text', 'input_schema': {'text': 'str'}, 'output_schema': 'List[str]', 'capabilities': ['topic-identification', 'relevance-scoring', 'bullet-generation']}, {'name': 'ContentExpander', 'version': '1.1.1', 'description': 'Expands bullet points or brief notes into full paragraphs', 'usage': 'Use this tool after KeypointExtractor to develop full content', 'input_schema': {'points': 'Union[List[str], str]'}, 'output_schema': 'str', 'capabilities': ['detail-addition', 'narrative-flow', 'tone-consistency']}], parameters={'content_type': 'article', 'tone': 'professional'}, embedding=array([-0.01680393, -0.01246005, -0.01141695, ..., -0.00973799,\n",
       "          0.00092522, -0.03249331], shape=(3072,))),\n",
       "  'similarity': np.float64(0.5079725852153282)},\n",
       " {'agent': AgentInfo(name='PDFContentWriter', description='Specialized agent for extracting text from PDFs and reformatting it into new content types', id='agent-013', tools=[{'name': 'TextExtractor', 'version': '2.0.3', 'description': 'Extracts text content from PDFs for repurposing', 'usage': 'Use this tool first when working with PDF files', 'input_schema': {'pdf_path': 'str'}, 'output_schema': 'str', 'capabilities': ['text-parsing', 'content-classification', 'key-fact-extraction']}, {'name': 'ContentReformatter', 'version': '1.4.1', 'description': 'Reformats extracted content into new document types', 'usage': 'Use this tool after TextExtractor to convert content to desired format', 'input_schema': {'text': 'str', 'format_description': 'str'}, 'output_schema': 'str', 'capabilities': ['blog-formatting', 'newsletter-creation', 'report-generation']}], parameters={'writing_style': 'engaging', 'seo_optimization': True}, embedding=array([-0.00254549, -0.03901929, -0.02073589, ..., -0.01276532,\n",
       "         -0.0109604 , -0.03923974], shape=(3072,))),\n",
       "  'similarity': np.float64(0.4893466289537307)},\n",
       " {'agent': AgentInfo(name='EmojiWriter', description='Specialized agent for converting plain text into emoji-rich content and creating emoji representations', id='agent-011', tools=[{'name': 'EmojiTranslator', 'version': '1.0.2', 'description': 'Translates words and concepts to relevant emojis', 'usage': 'Use this tool to convert text into matching emojis', 'input_schema': {'text': 'str'}, 'output_schema': 'str', 'capabilities': ['word-to-emoji', 'sentiment-emojis', 'emoji-storytelling']}, {'name': 'EmojiMixer', 'version': '0.9.1', 'description': 'Creates custom emoji combinations for content', 'usage': 'Use this tool to create creative emoji combinations from multiple concepts', 'input_schema': {'concepts': 'List[str]'}, 'output_schema': 'str', 'capabilities': ['emoji-chains', 'contextual-mixing', 'frequency-control']}], parameters={'density': 'medium', 'style': 'friendly'}, embedding=array([-0.03731536, -0.02685053, -0.01189685, ..., -0.02686431,\n",
       "         -0.01981432, -0.02897104], shape=(3072,))),\n",
       "  'similarity': np.float64(0.20284968864804662)}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents_with_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e455c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"<class 'inspect._empty'>\",\n",
       " 'agents_list': \"<class 'inspect._empty'>\",\n",
       " 'top_n': \"<class 'inspect._empty'>\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to get input schema for a tool function\n",
    "def get_input_schema(func):\n",
    "    \"\"\"Get the input schema for a function based on its signature.\"\"\"\n",
    "    signature = inspect.signature(func)\n",
    "    schema = {}\n",
    "    for param in signature.parameters.values():\n",
    "        schema[param.name] = str(param.annotation)\n",
    "    return schema\n",
    "\n",
    "get_input_schema(find_relevant_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebe864f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated function to map tool names to actual functions\n",
    "def get_tool_function(tool_name):\n",
    "    \"\"\"Map a tool name to its actual function implementation.\"\"\"  \n",
    "    # Map tools to their wrapped implementations\n",
    "    tool_map = {\n",
    "        \"DataframeLoader\": DataframeLoader.load_csv,\n",
    "        \"ContentExtractor\": ContentExtractor.extract_narrative,\n",
    "        \"EmojiTranslator\": EmojiTranslator.translate_to_emoji,\n",
    "        \"EmojiMixer\": EmojiMixer.create_emoji_mix,\n",
    "        \"KeypointExtractor\": KeypointExtractor.extract_keypoints,\n",
    "        \"ContentExpander\": ContentExpander().expand_content,\n",
    "        \"TextExtractor\": TextExtractor.extract_text_from_pdf,\n",
    "        \"ContentReformatter\": ContentReformatter.reformat_content,\n",
    "        \"MultilingualTranslator\": MultilingualTranslator().translate,\n",
    "        \"StylePreserver\": StylePreserver().preserve_style,\n",
    "    }\n",
    "    \n",
    "    return tool_map.get(tool_name)\n",
    "\n",
    "# Example usage of the tool function mapping\n",
    "tool_name = \"EmojiWriter\"\n",
    "get_tool_function(tool_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88234591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EmojiTranslator']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class for handling tool sequence generation\n",
    "class ToolsSeqFinder(BaseModel):\n",
    "    \"\"\"Model for finding the best sequence of tools to complete a task.\"\"\"\n",
    "    tools_name_in_seq: List[str]\n",
    "\n",
    "# Function to determine the best tool sequence for a task\n",
    "# Function to determine the best tool sequence for a task\n",
    "def determine_tool_sequence(agent, query):\n",
    "    \"\"\"Determine the best sequence of tools to use for completing a task.\"\"\"\n",
    "    # agent = agent['agent']\n",
    "    \n",
    "    message = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You have to find the best sequence for list of tools to complete the task. Available tools: {agent}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    tools_order = get_reply(message, ToolsSeqFinder)\n",
    "    # Access the attribute directly from the Pydantic model\n",
    "    return tools_order.tools_name_in_seq\n",
    "\n",
    "# Example usage of tool sequence determination\n",
    "agent = agents_with_embeddings[0]  # Use the first agent as an example\n",
    "tools_sequence = determine_tool_sequence(agent, query)\n",
    "tools_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "579d0710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gathered: 1. User requested key points to be extracted from a text about life and structured appropriately.\n",
      "2. User suggested a '--' transformation on all words in a text, similar to '--R--A--M--'.\n",
      "3. User's example specified 'ORIGINAL_TEXT' as 'ram' and 'TRANSFORMED_TEXT' as '--r--a--m--'.\n",
      "4. User confirmed wanting bullet points for keypoints structuring.\n",
      "Gathered inputs for StylePreserver: {\"original_text\": \"ram\", \"transformed_text\": \"--r--a--m--\"}\n"
     ]
    }
   ],
   "source": [
    "# Class for gathering information required by tools\n",
    "class ToolsInput(BaseModel):\n",
    "    \"\"\"Model for gathering tool input information.\"\"\"\n",
    "    information_tillnow: str\n",
    "    all_information_gathered: bool\n",
    "    flow_of_question: str\n",
    "\n",
    "# Improved function to gather input information for a tool\n",
    "def gather_tool_inputs(tool_name, tool_function, context=\"\", previous_outputs=None):\n",
    "    \"\"\"\n",
    "    Gather inputs required for a specific tool by asking user questions.\n",
    "    Returns a dictionary of inputs for the tool.\n",
    "    \"\"\"\n",
    "    if previous_outputs is None:\n",
    "        previous_outputs = {}\n",
    "    \n",
    "    # Get input schema for the tool function\n",
    "    input_schema = get_input_schema(tool_function)\n",
    "    \n",
    "    history = ''\n",
    "    \n",
    "    # Create initial message\n",
    "    message = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You have to ask for the details of the tools required to complete the task. The tool required is {tool_name} with inputs: {input_schema}.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"I want to use the {tool_name} tool. {context}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Set initial state\n",
    "    all_information_gathered = False\n",
    "    \n",
    "    # Interactive loop to gather all required inputs\n",
    "    while not all_information_gathered:\n",
    "        # Get the reply with questions\n",
    "        reply = get_reply(message, ToolsInput)\n",
    "        \n",
    "        # Use all_information_gathered attribute directly from the Pydantic model\n",
    "        all_information_gathered = reply.all_information_gathered\n",
    "        \n",
    "        if all_information_gathered:\n",
    "            break\n",
    "        else:\n",
    "            # Update the message with the flow of question\n",
    "            history = history + f\"Context:{context}\" + reply.flow_of_question + '\\n'\n",
    "            message[0]['content'] = f\"You have to ask for the details required to complete the task. The tool required is {tool_name} with inputs: {input_schema}. History of questions: {history}\"\n",
    "            \n",
    "            # Get input from user\n",
    "            query = input(f\"[Tool: {tool_name}] {reply.flow_of_question} \")\n",
    "            message[1]['content'] = query + f\"History of questions: {history}\"\n",
    "\n",
    "            # append the user query to the history\n",
    "            history += f\"User input: {query}\\n\"\n",
    "    \n",
    "    # Debug print\n",
    "    print(f\"Information gathered: {reply.information_tillnow}\")\n",
    "    \n",
    "    \n",
    "    # Convert user input to function input format\n",
    "    message = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Convert the user's input into a valid JSON object that matches this function schema: {input_schema}. Return ONLY the JSON object and nothing else.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"User's input: {reply.information_tillnow}. Create a JSON object that matches the function input schema.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    class FunctionInput(BaseModel):\n",
    "        function_input: str\n",
    "    \n",
    "    function_input = get_reply(message, FunctionInput)\n",
    "    \n",
    "    return function_input.function_input\n",
    "    \n",
    "# Example usage of gathering tool inputs\n",
    "tool_name = \"StylePreserver\"\n",
    "tool_function = get_tool_function(tool_name)\n",
    "context = \"hi\"\n",
    "tool_inputs = gather_tool_inputs(tool_name, tool_function, context)\n",
    "print(f\"Gathered inputs for {tool_name}: {tool_inputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27bf9a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool function for MultilingualTranslator: <function StylePreserver.preserve_style at 0x7cebc1b60b80>\n",
      "Executing MultilingualTranslator with inputs: {'text': 'I am sumit yadav', 'target_language': 'nepali'}\n"
     ]
    }
   ],
   "source": [
    "# Improved function to execute a tool with given inputs\n",
    "def execute_tool(tool_name, inputs):\n",
    "    \"\"\"Execute a tool with the given inputs and return the output.\"\"\"\n",
    "    # tool_function = get_tool_function(tool_name)\n",
    "    print(f\"Tool function for {tool_name}: {tool_function}\")\n",
    "    if not tool_function:\n",
    "        return f\"Error: Tool '{tool_name}' not found.\"\n",
    "    \n",
    "    # Debug information\n",
    "    print(f\"Executing {tool_name} with inputs: {inputs}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if inputs is a string and convert to dict if necessary\n",
    "        if isinstance(inputs, str):\n",
    "            inputs = yaml.safe_load(inputs)\n",
    "        elif not isinstance(inputs, dict):\n",
    "            raise ValueError(\"Inputs must be a dictionary or a YAML string.\")\n",
    "        # Call the tool function with the provided inputs\n",
    "        output = tool_function(**inputs)\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        return f\"Error executing tool '{tool_name}': {str(e)}\"\n",
    "    \n",
    "# Example usage of executing a tool\n",
    "tool_name = \"MultilingualTranslator\"\n",
    "inputs = {\n",
    "    \"text\": \"I am sumit yadav\",\n",
    "    \"target_language\": \"nepali\"\n",
    "}\n",
    "output = execute_tool(tool_name, inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
